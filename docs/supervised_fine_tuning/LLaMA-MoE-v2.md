# ğŸŒ´ Setup

dependencies:

cuda: 11.8
python: 3.11.4

Just follow the installation guide in [README.md](..%2F..%2FREADME.md), which can be simplified as:

```bash
conda create -n smoe python=3.11
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
pip install flash-attn --no-build-isolation
git clone git@github.com:pjlab-sys4nlp/llama-moe.git
cd llama-moe
pip install -e .[dev]
```

For special packages that are exclusive in v2, please follow:

1. Install `megablocks` and `stanford-stk`, the installation has to be done in the computing cluster: `srun -p MoE pip install megablocks==0.5.1`.
2. Install `scattermoe`: `git clone https://github.com/shawntan/scattermoe.git` and follow the instruction on the [official website](https://github.com/shawntan/scattermoe).

Finally, ensure that you environments satisfy:

```
deepspeed==0.14.4
flash-attn==2.6.1
megablocks==0.5.1
scattermoe==0.0.0 (installed locally)
torch==2.3.1
triton==2.3.1
transformers==4.42.4
```

For disabling `wandb` during training, you may run the following command:

```bash
$ wandb disable
```

## ğŸ—ƒï¸ Data Preparation

Run the following commands, and the data would be prepared in `resources/OpenHermes-2.5/openhermes2_5.jsonl` .

```bash
$ huggingface-cli download teknium/OpenHermes-2.5 --repo-type dataset --local-dir resources/OpenHermes-2.5 --local-dir-use-symlinks False
$ srun -p MoE python resources/OpenHermes-2.5/json2jsonl.py
```

## ğŸ§ƒ Model Preparation (Converting dense models to MoE)

Check and change the `num_experts`, `top_k`, `src_model_dir`, `tgt_model_dir_prefix`, and `tgt_moe_types` according to your settings.

After all settings are ready, run: `srun -p MoE python smoe/utils/expert_construction/convert_llama_to_mixtral.py` .

`tgt_moe_types`:
- modulelist: the raw and original implementation of Mixtral without 3rd-party training accelerations
- megablocks: enables the megablocks implementation: http://arxiv.org/abs/2211.15841
- scattermoe: the scattermoe implementation: http://arxiv.org/abs/2403.08245

## ğŸš€ Training

Check the settings in `scripts/v2_mixtral/mb_64e_top8.sh` and run `sbatch scripts/v2_mixtral/mb_64e_top8.sh` .

- `model_type` must be `auto` for enabling `trust_remote_code=True`.
- `##SBATCH` means the slurm setting is not activated.

## DPO

å…ˆè·‘`python smoe/entrypoint/dpo/merge_datasets.py`ï¼Œåˆå¹¶æ•°æ®é›†ã€‚æ³¨æ„æ•°æ®é›†åº”è¯¥æœ‰ç›¸åŒçš„columnsåç§°ï¼Œä¸ç„¶åˆå¹¶æ—¶ä¼šæŠ¥é”™ã€‚(è¿™é‡Œåº”è¯¥ä¹Ÿæœ‰æ›´å¥½çš„æ–¹æ³•ï¼Œä½†æ—¶é—´æœ‰é™ï¼Œæˆ‘å°±å…ˆè¿™æ ·ç®€å•å†™äº†ï¼Œä½ ä»¬åé¢æ­£å¼æçš„æ—¶å€™ï¼Œåº”è¯¥è¦å¯¹æ¯ç§æ•°æ®éƒ½åšä¸‹åˆ†åˆ«å¤„ç†)

è·‘è®­ç»ƒçš„è¯ï¼Œå°±ç›´æ¥`bash scripts/v2_mixtral/dpo/mb_64e_top8_dpo.sh`ã€‚å¯ä»¥è°ƒä»¥ä¸‹å‚æ•°ï¼š

```
output_router_logits: æ§åˆ¶æ˜¯å¦å°†balance lossè¿›è¡Œbackwardã€‚ç°åœ¨åªèƒ½ä¸ºFalseï¼Œå³ä¸åŠ balance lossã€‚ä¸ºTrueä¼šæŠ¥é”™ï¼Œåº”è¯¥è¦åŒæ­¥æ”¹Trainerï¼Œè¿™ä¸ªéœ€è¦ä½ ä»¬æ¥å®ç°ã€‚
freeze_gate: è¦ä¸è¦å†»ç»“gateå‚æ•°ã€‚ç°åœ¨ä¸ºTrueï¼Œæ¥å¼¥è¡¥ä¸åŠ balance losså¯¹å¹³è¡¡äº§ç”Ÿçš„å½±å“ã€‚
beta: DPOçš„ä¸€ä¸ªè¶…å‚ï¼Œä¸€èˆ¬åœ¨0.1åˆ°0.5ä¹‹é—´ï¼Œæˆ‘æ²¡è°ƒè¿™ä¸ªï¼Œç”¨çš„é»˜è®¤çš„0.1ï¼Œä¸è¿‡ä¹Ÿå¯ä»¥è°ƒä¸‹ã€‚
learning_rate: ä¸€èˆ¬è¦å°äºç­‰äºå¾®è°ƒé˜¶æ®µçš„æœ€ç»ˆå­¦ä¹ ç‡ï¼Œç›®å‰æœå‡ºæ¥8e-6æ¯”è¾ƒå¥½ï¼Œå¦‚æœåé¢å¾®è°ƒé˜¶æ®µå­¦ä¹ ç‡æœ‰ä¿®æ”¹ï¼Œå¯ä»¥ä»¥è¿™ä¸ªå€¼ä¸ºä¸­å¿ƒå†æœç´¢ä¸€æ³¢DPOå­¦ä¹ ç‡ã€‚
```

## ğŸ›« Evaluation

```bash
$ git clone https://github.com/EleutherAI/lm-evaluation-harness
$ cd lm-evaluation-harness
$ git checkout d14b36e81aea4cef
$ pip install -e .
# copy the scripts in smoe - `scripts/v2_mixtral/eval` to here
# change the model dir and evaluation settings
$ bash run.sh
```

## ğŸ—ºï¸ Roadmap & Instructions

- **balance loss**: to enable the balance loss, change the `output_router_logits` in a model's `config.json` to `true` (e.g. `resources/llama-3-8b-mixtral-megablocks-56e-top8/config.json`)
- **sequence length**: try to increase the `model_max_length` to `4096` as you can
- **megablocks & scattermoe**: there may be bugs and the evaluation results are bad than `modulelist`, but the training process is available with 2.6x acceleration and the loss goes down correctly
- **DPO Alignment**: Done
- **More Split Strategies**
- **Attention MoE**
- **More diversified & powerful data**
